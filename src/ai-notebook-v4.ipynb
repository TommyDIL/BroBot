{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U accelerate datasets peft transformers trl wandb bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:38.420347Z","iopub.execute_input":"2025-02-23T18:15:38.420727Z","iopub.status.idle":"2025-02-23T18:15:41.122004Z","shell.execute_reply.started":"2025-02-23T18:15:38.420700Z","shell.execute_reply":"2025-02-23T18:15:41.121107Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\n# Specify the checkpoint for SmolLM2 and set the device.\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n\n# Load the tokenizer and model.\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# For multi-GPU setups, consider using device_map=\"auto\":\nmodel = AutoModelForCausalLM.from_pretrained(\n        checkpoint,\n        device_map=\"auto\",  # {\"\": PartialState().process_index}\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:41.123661Z","iopub.execute_input":"2025-02-23T18:15:41.123963Z","iopub.status.idle":"2025-02-23T18:15:42.881298Z","shell.execute_reply.started":"2025-02-23T18:15:41.123937Z","shell.execute_reply":"2025-02-23T18:15:42.880758Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:42.883142Z","iopub.execute_input":"2025-02-23T18:15:42.883500Z","iopub.status.idle":"2025-02-23T18:15:42.888694Z","shell.execute_reply.started":"2025-02-23T18:15:42.883468Z","shell.execute_reply":"2025-02-23T18:15:42.888181Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolLM2-135M-Instruct', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:42.890158Z","iopub.execute_input":"2025-02-23T18:15:42.890505Z","iopub.status.idle":"2025-02-23T18:15:42.910522Z","shell.execute_reply.started":"2025-02-23T18:15:42.890473Z","shell.execute_reply":"2025-02-23T18:15:42.909839Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":65},{"cell_type":"markdown","source":"# Dataset\n\nJson structure output: https://huggingface.co/datasets/ChristianAzinn/json-training","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"TommyDIL/Bro-Cases\")\n# Perform Train-Test Split\nsplit_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n\n# Access train and test splits\ntrain_dataset = split_ds[\"train\"]\ntest_dataset = split_ds[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:42.911558Z","iopub.execute_input":"2025-02-23T18:15:42.911812Z","iopub.status.idle":"2025-02-23T18:15:43.604278Z","shell.execute_reply.started":"2025-02-23T18:15:42.911789Z","shell.execute_reply":"2025-02-23T18:15:43.603617Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:43.605210Z","iopub.execute_input":"2025-02-23T18:15:43.605528Z","iopub.status.idle":"2025-02-23T18:15:43.610917Z","shell.execute_reply.started":"2025-02-23T18:15:43.605497Z","shell.execute_reply":"2025-02-23T18:15:43.610272Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['case', 'verdict'],\n    num_rows: 3320\n})"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"# Set the response template to match the chat format.\n# (Ensure this string exactly matches the beginning of the assistant's response as output by apply_chat_template.)\nresponse_template = \"<|im_start|>assistant\\n\"\ninstruction_template = \"<|im_start|>user\\n\"\nPROMPT_TEMPLATE = \"\"\"You are an AI Judge specialized in interpreting and enforcing the sacred laws of the Bro Code. Given a case describing a social situation among Bros after <<<>>>, your role is to analyze the events and determine whether a violation of the Bro Code has occurred.\n\nUsing the official Bro Code as your reference, carefully assess the actions of the Bros involved. If a violation is found, clearly state which Bromandment or Article was broken and explain why. If the situation is ambiguous or does not constitute a clear violation, provide a reasoned verdict based on the principles of Bro justice.\n\nYour response should be structured as follows:\n\n    Verdict – State whether a violation has occurred.\n    Bro Code Reference – Cite the relevant Bromandment(s) or Article(s).\n    Explanation – Provide a clear and concise reasoning for the verdict.\n    Possible Consequences or Resolutions – If applicable, suggest an appropriate response (e.g., an apology, a duel, or invoking a Broll).\n\nMaintain a fair and neutral tone, upholding the spirit of brotherhood and justice while ensuring that Bros are held accountable to the sacred Code.\"\"\"\n\nUSER_PROMPT_TEMPLATE = \"\"\"<<<\nCase:\n{text}\n>>>\n\"\"\"\n\ndef formatting_prompts_func(example):\n    \"\"\"\n    Converts each example into a conversation string using the tokenizer's chat template.\n    Assumes each example contains lists under \"instruction\" and \"output\".\n    \"\"\"\n\n    output_texts = []\n\n    if not isinstance(example[\"case\"], list):\n        return []\n\n    for i in range(len(example[\"case\"])):\n        messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": PROMPT_TEMPLATE\n                },\n            {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(text=example[\"case\"][i])},\n            # Note: It is important that the assistant message content here does not\n            # include the assistant marker, because the chat template will insert it.\n            {\"role\": \"assistant\", \"content\": example[\"verdict\"][i]}\n        ]\n        # Use the chat template to generate the formatted text.\n        text = tokenizer.apply_chat_template(messages, tokenize=False)\n    \n        output_texts.append(text)\n\n    return output_texts\n\n\n# Create the data collator.\n# It will search for the response_template (here \"Assistant:\") in the formatted text\n# and ensure that only tokens after that marker contribute to the loss.\ncollator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n                                           instruction_template=instruction_template,\n                                           tokenizer=tokenizer,\n                                           mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:43.612098Z","iopub.execute_input":"2025-02-23T18:15:43.612458Z","iopub.status.idle":"2025-02-23T18:15:43.638169Z","shell.execute_reply.started":"2025-02-23T18:15:43.612407Z","shell.execute_reply":"2025-02-23T18:15:43.637531Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:113: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": \"\"\"You are an AI Judge specialized in interpreting and enforcing the sacred laws of the Bro Code. Given a case describing a social situation among Bros after <<<>>>, your role is to analyze the events and determine whether a violation of the Bro Code has occurred.\n\nUsing the official Bro Code as your reference, carefully assess the actions of the Bros involved. If a violation is found, clearly state which Bromandment or Article was broken and explain why. If the situation is ambiguous or does not constitute a clear violation, provide a reasoned verdict based on the principles of Bro justice.\n\nYour response should be structured as follows:\n\n    Verdict – State whether a violation has occurred.\n    Bro Code Reference – Cite the relevant Bromandment(s) or Article(s).\n    Explanation – Provide a clear and concise reasoning for the verdict.\n    Possible Consequences or Resolutions – If applicable, suggest an appropriate response (e.g., an apology, a duel, or invoking a Broll).\n\nMaintain a fair and neutral tone, upholding the spirit of brotherhood and justice while ensuring that Bros are held accountable to the sacred Code.\"\"\"},\n    {\"role\": \"user\", \"content\": \"\"\"<<<\nCase:\nBro Ethan agreed to be Bro Jake’s wingman at a party. However, instead of helping Jake talk to his crush, Ethan ended up flirting with her himself and later left the party with her.\n>>>\"\"\"},\n    {\"role\": \"assistant\", \"content\": \"\"\"Verdict: Violation of the Bro Code.\nBro Code Reference: Bromandment I – Bros before hoes; Article 3 – The Conquest Clause.\nExplanation: As a wingman, Bro Ethan had a duty to support Bro Jake, not compete with him. By making a move on Jake’s crush, he directly violated both the principle of brotherly loyalty and the Conquest Clause.\nPossible Consequences or Resolutions: Bro Ethan should apologize and recognize his failure as a wingman. If the situation is severe, Bro Jake may challenge him to a Duel of Conquest (Article 5).\"\"\"}\n    ], tokenize=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:43.640516Z","iopub.execute_input":"2025-02-23T18:15:43.640719Z","iopub.status.idle":"2025-02-23T18:15:43.662814Z","shell.execute_reply.started":"2025-02-23T18:15:43.640700Z","shell.execute_reply":"2025-02-23T18:15:43.662294Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are an AI Judge specialized in interpreting and enforcing the sacred laws of the Bro Code. Given a case describing a social situation among Bros after <<<>>>, your role is to analyze the events and determine whether a violation of the Bro Code has occurred.\\n\\nUsing the official Bro Code as your reference, carefully assess the actions of the Bros involved. If a violation is found, clearly state which Bromandment or Article was broken and explain why. If the situation is ambiguous or does not constitute a clear violation, provide a reasoned verdict based on the principles of Bro justice.\\n\\nYour response should be structured as follows:\\n\\n    Verdict – State whether a violation has occurred.\\n    Bro Code Reference – Cite the relevant Bromandment(s) or Article(s).\\n    Explanation – Provide a clear and concise reasoning for the verdict.\\n    Possible Consequences or Resolutions – If applicable, suggest an appropriate response (e.g., an apology, a duel, or invoking a Broll).\\n\\nMaintain a fair and neutral tone, upholding the spirit of brotherhood and justice while ensuring that Bros are held accountable to the sacred Code.<|im_end|>\\n<|im_start|>user\\n<<<\\nCase:\\nBro Ethan agreed to be Bro Jake’s wingman at a party. However, instead of helping Jake talk to his crush, Ethan ended up flirting with her himself and later left the party with her.\\n>>><|im_end|>\\n<|im_start|>assistant\\nVerdict: Violation of the Bro Code.\\nBro Code Reference: Bromandment I – Bros before hoes; Article 3 – The Conquest Clause.\\nExplanation: As a wingman, Bro Ethan had a duty to support Bro Jake, not compete with him. By making a move on Jake’s crush, he directly violated both the principle of brotherly loyalty and the Conquest Clause.\\nPossible Consequences or Resolutions: Bro Ethan should apologize and recognize his failure as a wingman. If the situation is severe, Bro Jake may challenge him to a Duel of Conquest (Article 5).<|im_end|>\\n'"},"metadata":{}}],"execution_count":69},{"cell_type":"markdown","source":"# Lora Config","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\n\n# Note that r, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation.\n# A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation.\n# This can lead to faster training and potentially reduced computational requirements.\n# However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases.\n# This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\nlora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:43.664031Z","iopub.execute_input":"2025-02-23T18:15:43.664228Z","iopub.status.idle":"2025-02-23T18:15:43.684831Z","shell.execute_reply.started":"2025-02-23T18:15:43.664210Z","shell.execute_reply":"2025-02-23T18:15:43.684223Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"# Wandb\n\nCreate token and account: https://wandb.ai/home","metadata":{}},{"cell_type":"code","source":"import wandb\nimport getpass\n\ntoken = getpass.getpass()\nwandb.login(key=token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T18:15:43.685751Z","iopub.execute_input":"2025-02-23T18:15:43.686046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hub_model_id = \"TommyDIL/BroBot\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n\n# setup the trainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        args=SFTConfig(\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                warmup_steps=100,\n                max_steps=1000,\n                learning_rate=0.0002,\n                lr_scheduler_type=\"cosine\",\n                eval_strategy=\"steps\",\n                eval_steps=150,\n                weight_decay=0.01,\n                bf16=True,\n                logging_strategy=\"steps\",\n                logging_steps=10,\n                output_dir=\"./\" + OUTPUT_DIR,\n                optim=\"paged_adamw_8bit\",\n                seed=42,\n                run_name=f\"train-{OUTPUT_DIR}\",\n                report_to=\"wandb\",\n                save_steps=31,\n                hub_model_id=hub_model_id,\n                save_total_limit=4,\n                ),\n        peft_config=lora_config,\n        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SFT Trainer config","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import is_torch_xpu_available, is_torch_npu_available\nimport torch\n\n# Lancement du processus d'entraînement du modèle.\n# Ici, 'trainer.train()' déclenche la phase de fine-tuning,\n# dans laquelle les paramètres du modèle sont ajustés sur une tâche spécifique\n# en utilisant des données d'entraînement pertinentes.\ntrainer.train()\n\n# Une fois l'entraînement terminé, on sauvegarde l'adaptateur LoRA (fine-tuning léger).\n# LoRA (Low-Rank Adaptation) est une technique destinée à fine-tuner les grands\n# modèles en modifiant uniquement un sous-ensemble restreint de paramètres.\nfinal_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\ntrainer.model.save_pretrained(final_checkpoint_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.push_to_hub(dataset_name=\"TommyDIL/Bro-Cases\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nettoyage des ressources mémoire pour libérer l'espace GPU ou autres accélérateurs,\n# ce qui est utile avant de fusionner l'adaptateur LoRA avec le modèle de base.\ndel model  # Suppression explicite du modèle de la mémoire.\n\n# Vider les caches des accélérateurs (XPU, NPU ou GPU en fonction de la disponibilité).\n# Cela optimise l'utilisation future des ressources.\nif is_torch_xpu_available():\n    torch.xpu.empty_cache()  # Vide les caches spécifiques pour XPU.\nelif is_torch_npu_available():\n    torch.npu.empty_cache()  # Vide les caches spécifiques pour NPU.\nelse:\n    torch.cuda.empty_cache()  # Vide les caches GPU standard.\n\n# Chargement du modèle adapté (en incluant l'adaptateur LoRA) pour effectuer une fusion\n# avec le modèle de base. Cela permet de sauvegarder un modèle autonome optimisé.\nfrom peft import AutoPeftModelForCausalLM\n\n# Chargement du modèle préalablement sauvegardé depuis le répertoire OUTPUT_DIR.\n# Les paramètres 'device_map' et 'torch_dtype' permettent d'optimiser le chargement :\n# - 'device_map=\"auto\"' ajuste automatiquement le placement sur le GPU, CPU ou autre.\n# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\n#    la mémoire nécessaire tout en maintenant des performances stables.\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n        OUTPUT_DIR,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16\n        )\n\n# Fusion de l'adaptateur LoRA directement dans le modèle de base,\n# afin de produire un modèle final unique tout en réduisant ses redondances.\nmodel = model.merge_and_unload()\n\n# Sauvegarde du modèle fusionné dans un répertoire spécifique.\n# 'safe_serialization=True' garantit que le modèle est stocké au format sûr,\n# pour une compatibilité future et une intégrité des données.\noutput_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(hub_model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_json_schema = \"\"\"{\n  \"type\": \"object\",\n  \"properties\": {\n    \"weather_data\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"year\": { \"type\": \"integer\" },\n          \"station\": { \"type\": \"string\" },\n          \"temperature\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"min\": { \"type\": \"number\" },\n              \"max\": { \"type\": \"number\" }\n            },\n            \"required\": [\"min\", \"max\"]\n          },\n          \"events\": {\n            \"type\": \"array\",\n            \"items\": { \"type\": \"string\" }\n          }\n        },\n        \"required\": [\"year\", \"station\", \"temperature\", \"events\"]\n      }\n    },\n    \"required\": [\"weather_data\"]\n  }\n}\"\"\"\n\ntest_query = \"Provide a detailed breakdown of meteorological data recorded in the city of Berlin from 2015 to 2020. The data should include the year, meteorological station, temperature ranges (minimum and maximum), and any significant events.\"\n\ntest_response = \"\"\"{\n  \"weather_data\": [\n    {\n      \"year\": 2015,\n      \"station\": \"Berlin Central Station\",\n      \"temperature\": { \"min\": -5.2, \"max\": 35.1 },\n      \"events\": [\"Heavy snowfall in January\", \"Heatwave in July\"]\n    },\n    {\n      \"year\": 2017,\n      \"station\": \"Berlin East Station\",\n      \"temperature\": { \"min\": -4.0, \"max\": 32.8 },\n      \"events\": [\"Thunderstorms in April\", \"Flooding in June\"]\n    },\n    {\n      \"year\": 2020,\n      \"station\": \"Berlin West Station\",\n      \"temperature\": { \"min\": -3.9, \"max\": 36.5 },\n      \"events\": [\"Drought in September\", \"Blizzards in February\"]\n    }\n  ]\n}\"\"\"\n\nmessages = [\n    {\n        \"role\":    \"system\",\n        \"content\": \"You are are an expert in generate json structure based on user query and schema.\"\n        },\n    {\n        \"role\":    \"user\",\n        \"content\": PROMPT_TEMPLATE.format(query=test_query, schema=test_json_schema)\n        },\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\nprint(\"----------------- Generated text -----------------\")\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}